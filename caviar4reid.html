
<!-- saved from url=(0032)http://www.cs.berkeley.edu/~rbg/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>CAVIAR4REID dataset </title>
<style type="text/css" media="screen">
html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

strong, b {
	font-weight:bold;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 200px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 170px;
}

div.dissert {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.dissert div {
  padding-left: 150px;
}

img.dissert {
  margin-bottom: 0.5em;
  float: left;
  width: 140px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}

</style>

<script type="text/javascript" async="" src="./page_files/ga.js"></script><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<script type="text/javascript" src="./page_files/hidebib.js"></script>

<link href="./page_files/css" rel="stylesheet" type="text/css">
<!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>-->
<!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>-->
<!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
<style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>

<body>


<div class="section">
<h2 id="CAVIAR">CAVIAR4REID dataset</h2>
<div class="paper" id="Cheng:BMVC11">
  <img class="paper" title="BMVC 2011" src="./page_files/CPS_BMVC.png">
  <div>
    <a class="paper" href="http://lorisbaz.github.io/papers/proceedings/Chengetal_BMVC11.pdf"> Custom pictorial structures for re-identification</a><br>
    D. S. Cheng, M. Cristani, M. Stoppa, <strong>L. Bazzani</strong>, V. Murino<br>
     In British Machine Vision Conference (BMVC), 2011 <br>
    <a href="http://lorisbaz.github.io/caviar4reid.html">CAVIAR4REID dataset</a> / <a href="http://www.youtube.com/watch?v=wNkbwCgxJ6o">video</a> / <a shape="rect" href="javascript:togglebib('Cheng:BMVC11')" class="togglebib">bibtex</a>
    <pre xml:space="preserve" style="display: none;">
@inproceedings{Cheng:BMVC11,
  title = {Custom Pictorial Structures for Re-identification},
  author = {Cheng, D. S. and Cristani, M. and Stoppa, M. and
        Bazzani, L. and Murino, V.},
  booktitle = {British Machine Vision Conference (BMVC)},
  year = {2011}
}
    </pre>
<span class="blurb"> Cite this paper if you use CAVIAR4REID <span>
  </div>
  <div class="spanner"></div>
</div>
</div>

<h3 id="SDALF">Details</h3>
<div class="paper">
CAVIAR4REID is a dataset for evaluating person re-identification algorithms. As the name suggest, the dataset has been extracted from the CAVIAR dataset mostly famous for person tracking and detection evaluations (available <a href="http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/">here</a>). <br><br>
<center><h3><a href="http://lorisbaz.github.io/datasets/CAVIAR4REID.zip">Download CAVIAR4REID</a></h3>

  <a href="http://lorisbaz.github.io/datasets/CAVIAR4REID.zip"><img title="CAVIAR4REID 2011" src="http://lorisbaz.github.io/page_files/mosaic_pedestrian.jpg"></a>
</center>
</div>

</div>
<div class="section">
<h3>How we have built it</h3>
<div class="paper">
The original dataset, CAVIAR, consists of several sequences filmed in the entrance lobby of the INRIA Labs and in a shopping centre in Lisbon. We selected the shopping centre scenario, because it is a less controlled recording and also the cameras are better located (in INRIA Labs scenario, the camera is located overhead. Not a typical scenario for re-identification.). Shopping centre dataset contains 26 sequences recorded from two different points of view at the resolution of 384 X 288 pixels. It includes people walking alone, meeting with others, window shopping, entering and exiting shops. The ground truth has been used to extract the bounding box of each pedestrian. Then we manual select a total of 72 pedestrians: 50 of them with both the camera views and the remaining 22 with one camera view. For each pedestrian, we accurately selected a set of images for each camera view (where available) in order to maximize the variance with respect to resolution changes, light conditions, occlusions, and pose changes so as to make challenging the re-identification task.
</div>

</div>
<div class="section">
<h3>Why Yet Another Re-identification Dataset</h3>
<div class="paper">
There exist many publicly available dataset, such as:
<a href="http://vision.soe.ucsc.edu/?q=node/178">VIPeR</a>,
<a href="http://homepages.dcc.ufmg.br/~william/datasets.html">ETHZ</a> and iLIDS. However, none of them mirror a real re-identification scenario. Even thought VIPeR is one the most promising and challenging dataset in this field, it contains only a image for each individuals. Nowadays we have powerful tools for person tracking, so we can accumulate images over time to make easier the problem. So, we need a dataset that contains multiple images for each person. iLIDS and ETHZ seems to deal with this problem, however they contain images not captured from different cameras and also the intra-person images do not vary a lot in terms of resolution, light, pose, and so on. <br>
Among this dataset, the only one that contains this requirements is CAVIAR4REID: 1) it has broad changes in resolution, the minimum and maximum size of the images is 17 X 39 and 72 X 144, respectively. 2) Unlike ETHZ and iLIDS, it is extracted from a real scenario where re-identification is necessary due to the presence of multiple cameras and 3) pose variations are severe. 4) Unlike VIPeR, it contains more than one image for each view. 5) It contains the union of all the images variations of the other datasets.
</div>

<div class="section">
<h3>Technical info</h3>
<div class="paper">
The zip contains the dataset as a set of images. For each person we have a set of 5 or 10 images. The filename identifies which images are associated to each person:
<br>
XXXXYYY.jpg
<br>
  XXXX = identifier of the person
<br>
  YYY = identifier of the image for that specific person
<br>
E.g.: 0003005.jpg is the 5th image of the 3rd person
</div>


</div>

<div style="clear:both;">
  <p align="right"><font size="2"><a href="http://lorisbaz.github.io"><< Go back to main page</a></font></p><br>
</div>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>




</body></html>
